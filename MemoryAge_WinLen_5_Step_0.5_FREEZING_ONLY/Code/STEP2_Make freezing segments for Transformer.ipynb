{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51b921eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3096f468",
   "metadata": {},
   "outputs": [],
   "source": [
    "mice = ['ym212','ym213','ym214','ym215','ym218','ym219','ym220','ym222','ym223','ym224','ym226','ym227']  # 12 in total\n",
    "sessions = ['5FC','7FC']  # recent and remote\n",
    "data_dir = r'/Users/david/Projects/Yuichi/MemoryAge_WinLen_2.56_Step_0.1_FREEZING_ONLY/Data'\n",
    "\n",
    "sample_rate = 1600\n",
    "window_len = int(sample_rate*2.56)\n",
    "step = int(sample_rate*0.1)        \n",
    "Z_SCORE = True\n",
    "\n",
    "PRINT = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define functions for the feature extraction in Attention analysis\n",
    "function 'calculate_features' will process each small segment (3 * 256) of a LFP (3 * 4096) from a mouse in one session, and return 24 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET PARAMS\n",
    "sample_rate = 1600  # Hz\n",
    "\n",
    "# freq bands\n",
    "freq_bands = {\n",
    "    'theta': (6, 12),\n",
    "    'beta': (20, 30),\n",
    "    'sGamma': (30, 50),\n",
    "    'fGamma': (60, 90)\n",
    "}\n",
    "\n",
    "chunk_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter the signal\n",
    "def bandpass_filter(signal, band, sample_rate):\n",
    "    '''\n",
    "    signal: shape (n_channels, n_times), e.g., (3, 256)\n",
    "    band: low and high cutoff frequencies, e.g., (6, 12)\n",
    "    sample_rate: int, e.g., 1600Hz\n",
    "\n",
    "    output: each row is a filtered signal\n",
    "    '''\n",
    "    nyquist = 0.5 * sample_rate\n",
    "    low, high = band\n",
    "\n",
    "    # Normalize the frequency to the Nyquist\n",
    "    low = low / nyquist\n",
    "    high = high / nyquist\n",
    "    # build a bandpass Butterworth filter with order 4\n",
    "    b, a = scipy.signal.butter(4, [low, high], btype='band')\n",
    "\n",
    "    # Apply the filter to the signal\n",
    "    # important: axis=1: filter along the columns\n",
    "    return scipy.signal.filtfilt(b, a, signal, axis=1)\n",
    "\n",
    "\n",
    "# Function to calculate the envelop of the signal\n",
    "def calculate_envelop(signal):\n",
    "    '''\n",
    "    signal: filtered signal with shape (n_channels, n_times), e.g., (3, 256)\n",
    "    axis: = 1, calculate envelop along the columns\n",
    "    '''\n",
    "    analytic_signal = scipy.signal.hilbert(signal, axis=1)\n",
    "    return np.abs(analytic_signal)\n",
    "\n",
    "\n",
    "# Function to calculate the PSD\n",
    "def calculate_psd(signal, sample_rate):\n",
    "    freqs, psd = scipy.signal.welch(signal, sample_rate, axis=1)\n",
    "    return freqs, psd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features(LFP, freq_bands, sample_rate):\n",
    "\n",
    "    # ACC_theta, CA1_theta, BLA_theta, ACC_beta, CA1_beta, BLA_beta, ACC_sGamma, CA1_sGamma, BLA_sGamma, ACC_fGamma, CA1_fGamma, BLA_fGamma\n",
    "    psd_features = []\n",
    "\n",
    "    # ACC-CA1-theta_corr, ACC-BLA-theta_corr, CA1-BLA-theta_corr, ACC-CA1-beta_corr, ACC-BLA-beta_corr, CA1-BLA-beta_corr, \n",
    "    # ACC-CA1-sGamma_corr, ACC-BLA-sGamma_corr, CA1-BLA-sGamma_corr, ACC-CA1-fGamma_corr, ACC-BLA-fGamma_corr, CA1-BLA-fGamma_corr\n",
    "    corr_features = []\n",
    "\n",
    "    for band, (low, high) in freq_bands.items():   # band = 'theta', 'beta', 'sGamma', 'fGamma'\n",
    "        # ----- Filtering each channel\n",
    "        filtered_signal = bandpass_filter(LFP, (low, high), sample_rate)  # filtered_signal.shape = (3, 256)\n",
    "        # ----- Calculating the envelop of each filtered signal of each channel\n",
    "        envelop = calculate_envelop(filtered_signal)                      # envelop.shape = (3, 256)\n",
    "\n",
    "        # exclude 15 time points in the beginning and the end of the filtered signal and its envelop\n",
    "        # becasue the envelops have distortions at the beginning and the end\n",
    "        exclude_edge = 15\n",
    "        \n",
    "        # ----- Calculate mean PSD in the freq range\n",
    "        freqs, psd = calculate_psd(filtered_signal, sample_rate)\n",
    "        # freqs.shape = (129,), psd.shape = (3, 129)\n",
    "        for ch_i in range(psd.shape[0]):\n",
    "            # select the psd in the freq range (low, high)\n",
    "            idx = np.logical_and(freqs >= low, freqs <= high)\n",
    "            # get the mean psd in the freq range\n",
    "            mean_psd = np.mean(psd[ch_i, idx])\n",
    "            psd_features.append(mean_psd)\n",
    "\n",
    "        # Calculating correlation\n",
    "        # we need to exclude the edge of the envelops because they have distortions\n",
    "        # corrcoef will calculate the correlation between each pair of channels\n",
    "        corr_matrix = np.corrcoef(envelop[:,exclude_edge:-exclude_edge])\n",
    "        # append corr_matrix[0,1], corr_matrix[0,2], corr_matrix[1,2]\n",
    "        corr_features.extend(corr_matrix[np.triu_indices(3, k=1)])\n",
    "\n",
    "    features = np.array(psd_features + corr_features)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05c5f9f5",
   "metadata": {},
   "source": [
    "# 2. The original code for Freezing segments creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecf8a8bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81972da523374befb3cbec6212470fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mouse:ym212 Session:5FC| There are 1076 segments.\n",
      "            Session:7FC| There are 452 segments.\n",
      "            Total:1528\n",
      "Mouse:ym213 Session:5FC| There are 274 segments.\n",
      "            Session:7FC| There are 443 segments.\n",
      "            Total:717\n",
      "Mouse:ym214 Session:5FC| There are 142 segments.\n",
      "            Session:7FC| There are 344 segments.\n",
      "            Total:486\n",
      "Mouse:ym215 Session:5FC| There are 1608 segments.\n",
      "            Session:7FC| There are 442 segments.\n",
      "            Total:2050\n",
      "Mouse:ym218 Session:5FC| There are 1601 segments.\n",
      "            Session:7FC| There are 1075 segments.\n",
      "            Total:2676\n",
      "Mouse:ym219 Session:5FC| There are 288 segments.\n",
      "            Session:7FC| There are 153 segments.\n",
      "            Total:441\n",
      "Mouse:ym220 Session:5FC| There are 1597 segments.\n",
      "            Session:7FC| There are 745 segments.\n",
      "            Total:2342\n",
      "Mouse:ym222 Session:5FC| There are 587 segments.\n",
      "            Session:7FC| There are 1936 segments.\n",
      "            Total:2523\n",
      "Mouse:ym223 Session:5FC| There are 1047 segments.\n",
      "            Session:7FC| There are 1755 segments.\n",
      "            Total:2802\n",
      "Mouse:ym224 Session:5FC| There are 425 segments.\n",
      "            Session:7FC| There are 256 segments.\n",
      "            Total:681\n",
      "Mouse:ym226 Session:5FC| There are 842 segments.\n",
      "            Session:7FC| There are 226 segments.\n",
      "            Total:1068\n",
      "Mouse:ym227 Session:5FC| There are 797 segments.\n",
      "            Session:7FC| There are 505 segments.\n",
      "            Total:1302\n",
      "There are 10284 segments in recent.\n",
      "There are 8332 segments in remote.\n"
     ]
    }
   ],
   "source": [
    "tot_num_seg_recent = 0\n",
    "tot_num_seg_remote = 0\n",
    "\n",
    "for mouse in tqdm(mice):\n",
    "    for session in sessions:\n",
    "        # ---------------------------------------------------\n",
    "        # load LFP and the corresponding TS\n",
    "        lfp_name = data_dir + '/' + mouse + '_' + session + '_' + 'LFP.mat'\n",
    "        lfp_and_ts = sio.loadmat(lfp_name)\n",
    "        lfp = lfp_and_ts['LFP_3_regions'].transpose()   # e.g., shape=(3, 987136)\n",
    "        ts = lfp_and_ts['LFP_ts_usec'].squeeze()        # e.g., shape=(987136)\n",
    "        del lfp_and_ts\n",
    "        \n",
    "        # load (fB, fE)\n",
    "        fre_ts_name = data_dir + '/' + mouse + '_' + session + '_' + 'Freeze_Ts.csv'\n",
    "        fre_B_E = pd.read_csv(fre_ts_name, header=None)\n",
    "        fre_B_E = fre_B_E.rename(columns={0:'fB',1:'fE'})\n",
    "\n",
    "        # check whether 'the 1st start timestamp of freeze behavior' happens later than 'LFP timestamp start'\n",
    "        if fre_B_E.iloc[0,0] < ts[0]:\n",
    "            print(f'Attention: Mouse{mouse}, Session{session}.')\n",
    "        # check whether 'the last end timestamp of freeze behavior' happens earlier than 'LFP timestamp end'\n",
    "        if fre_B_E.iloc[-1,1] > ts[-1]:\n",
    "            print(f'Attention: Mouse{mouse}, Session{session}.')\n",
    "        # After check, we can conclude that all the timestamps in fre_B_E is in the range of ts\n",
    "\n",
    "        # Now, we have\n",
    "        # - lfp           in millivolts\n",
    "        # - ts            in micro second, the lfp is the downsampling signal with a sample rate=1600Hz\n",
    "        #                 any two adjacent ts have a diff 625us  \n",
    "        #                 e.g., np.diff(ts[:,0]) \n",
    "        # - fre_B_E   in micro second\n",
    "\n",
    "        # ---------------------------------------------------\n",
    "        #   Z-SCORE normalization\n",
    "        # ---------------------------------------------------\n",
    "        if Z_SCORE:\n",
    "            freeze_lfp_mean_std = pd.read_csv(r'./check_freeze_lfp_mean_std_figs/freeze_lfp_mean_std.csv')\n",
    "            freeze_lfp_mean_std = freeze_lfp_mean_std.set_index(['Mouse-Session'])\n",
    "\n",
    "            # the mean and std of the current session freezing lfp\n",
    "            lfp_mean = freeze_lfp_mean_std.loc[mouse + '_' + session, 'Mean']\n",
    "            lfp_std = freeze_lfp_mean_std.loc[mouse + '_' + session, 'Std']\n",
    "\n",
    "            lfp = (lfp - lfp_mean)/lfp_std\n",
    "\n",
    "            # Now, we have\n",
    "            # - lfp        with freezing period z-score normalized\n",
    "            #              if you only get the freezing segments, then the mean=0, std=1\n",
    "            # TODO: How to deal with the outliers?\n",
    "\n",
    "        # ---------------------------------------------------\n",
    "        # Convert the fB fE from machine time to the corresponding idx on lfp array\n",
    "        # e.g., fB machine time -> lfp machine time -> lfp array idx\n",
    "\n",
    "        # we make an identical df as fre_B_E\n",
    "        # this df is used to save each fB or fE's idx on lfp data array\n",
    "        fre_B_E_idx = fre_B_E.copy()\n",
    "        fre_B_E_idx['fB'] = np.nan\n",
    "        fre_B_E_idx['fE'] = np.nan\n",
    "\n",
    "        ts_starting_point = 0\n",
    "        # note that too many loops is not efficient, but consider the small data scale, it is fine.\n",
    "        # the time complicity is O(ts.shape[0])\n",
    "        for row_i in range(len(fre_B_E)):    \n",
    "            for col_j in range(2):\n",
    "                behav_ts = fre_B_E.iloc[row_i, col_j]\n",
    "\n",
    "                for ts_idx in range(ts_starting_point, ts.shape[0]):\n",
    "                    if behav_ts <= ts[ts_idx]:\n",
    "                        fre_B_E_idx.iloc[row_i, col_j] = ts_idx\n",
    "                        ts_starting_point = ts_idx + 1\n",
    "                        break  # 'break' apply to the innermost loop.\n",
    "\n",
    "        # make sure the indice are int type\n",
    "        fre_B_E_idx = fre_B_E_idx.astype(int)\n",
    "        # fE - fB to get the length of each period\n",
    "        fre_periods = fre_B_E_idx.iloc[:,1] - fre_B_E_idx.iloc[:,0]\n",
    "        # remove all the periods smaller than 'window_len' defined above\n",
    "        periods_keep = window_len <= fre_periods  # for example, remove all the periods smaller than 2.56s\n",
    "        fre_B_E_idx = fre_B_E_idx.loc[periods_keep, :].reset_index(drop=True)\n",
    "\n",
    "        del row_i, col_j, behav_ts, ts_idx, ts_starting_point, fre_periods, periods_keep\n",
    "        # Now, we have\n",
    "        # - fre_B_E_idx\n",
    "        \n",
    "        # ---------------------------------------------------\n",
    "        # now we make each 'window_len' lfp using freezing timestamps\n",
    "\n",
    "        segment_all = []\n",
    "        feature_all = []\n",
    "\n",
    "        # for each Begin End pair\n",
    "        for B_E_pair_idx in range(len(fre_B_E_idx)):\n",
    "            # get the Begin End index\n",
    "            B_tmp = fre_B_E_idx.iloc[B_E_pair_idx, 0]\n",
    "            E_tmp = fre_B_E_idx.iloc[B_E_pair_idx, 1]\n",
    "\n",
    "            for segment_start in range(B_tmp, E_tmp - window_len + 2, step):\n",
    "            # why do I set the ending point to E_tmp - window_len + 2?\n",
    "            # because even if you get the last value, which is E_tmp - window_len + 1, it is still ok.\n",
    "            # because from E_tmp - window_len + 1 to E_tmp (inclusive), there are window_len points! \n",
    "\n",
    "                # cut lfp\n",
    "                segment = lfp[:, segment_start: segment_start + window_len]\n",
    "                segment_all.append(segment)\n",
    "\n",
    "                # ----------- UPDATE for the attention analysis -----------\n",
    "                feature_1_sample = []\n",
    "                # for each small segment with the len of 256 (transformer's chunk size), calculate the features\n",
    "                for chunk_i in range(0, window_len, chunk_size):\n",
    "                    # get the small segment\n",
    "                    segment_small = segment[:, chunk_i:chunk_i+chunk_size]\n",
    "                    # calculate the features\n",
    "                    feature_1_small_segment = calculate_features(segment_small, freq_bands, sample_rate)  # shape=(24,)\n",
    "                    feature_1_sample.append(feature_1_small_segment)\n",
    "                \n",
    "                feature_all.append(np.stack(feature_1_sample))  # each time append a (16,24) array for one sample\n",
    "                # -------------------------------------------------------------------\n",
    "        \n",
    "        if session == '5FC':  # recent\n",
    "            tot_num_seg_recent += len(segment_all)\n",
    "            if PRINT:\n",
    "                print(f'Mouse:{mouse} Session:{session}| There are {len(segment_all)} segments.')\n",
    "                recent_seg_num_this_mouse = len(segment_all)\n",
    "\n",
    "            # make segment_all has the shape of segments*channels*time\n",
    "            segment_all = np.stack(segment_all)\n",
    "            # save as pickle\n",
    "            file_name = mouse + '_Recent'\n",
    "            with open(os.path.join(data_dir, 'pickle_for_transformer', file_name), 'wb') as f:\n",
    "                pickle.dump(segment_all, f)\n",
    "            \n",
    "            with open(os.path.join(data_dir, 'features_for_attention_analysis', file_name), 'wb') as f:\n",
    "                pickle.dump(feature_all, f) \n",
    "\n",
    "        else:                 # remote\n",
    "            tot_num_seg_remote += len(segment_all)\n",
    "            if PRINT:\n",
    "                print(f'            Session:{session}| There are {len(segment_all)} segments.')\n",
    "                remote_seg_num_this_mouse = len(segment_all)\n",
    "                print(f'            Total:{recent_seg_num_this_mouse+remote_seg_num_this_mouse}')\n",
    "\n",
    "            # make segment_all has the shape of segments*channels*time\n",
    "            segment_all = np.stack(segment_all)\n",
    "            # save as pickle\n",
    "            file_name = mouse + '_Remote'\n",
    "            with open(os.path.join(data_dir, 'pickle_for_transformer', file_name), 'wb') as f:\n",
    "                pickle.dump(segment_all, f)\n",
    "            \n",
    "            with open(os.path.join(data_dir, 'features_for_attention_analysis', file_name), 'wb') as f:\n",
    "                pickle.dump(feature_all, f)\n",
    "\n",
    "print(f'There are {tot_num_seg_recent} segments in recent.')\n",
    "print(f'There are {tot_num_seg_remote} segments in remote.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "e05041afb5fb97d60f34017cad474a5fbc19fa86586c1a1519b5e4b0932f3ec4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
